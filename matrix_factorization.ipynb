{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecommendationSystemnm941.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtIRE_Wm15Jd",
        "colab_type": "text"
      },
      "source": [
        "# CS 550 - Massive Data Mining \n",
        "# Recommendation Systems\n",
        "## Team Members\n",
        "### 1. Rushabh Bid ()\n",
        "### 2. Fatima AlSaadeh ()\n",
        "### 3. Keya Desai ()\n",
        "### 4. Naveen Narayanan Meyyappan (nm941)\n",
        "\n",
        "#### Data set: Movie review data set from \"https://grouplens.org/datasets/movielens/latest/\" \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS5Llh0f4je4",
        "colab_type": "text"
      },
      "source": [
        "## Data selection and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POjKvl1ZokV3",
        "colab_type": "code",
        "outputId": "2d392a2f-8dbd-470e-d06d-91024dbb474d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Installing the required packages\n",
        "pip install surprise"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: surprise in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (from surprise) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.18.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg-kJNvax1nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the required libraries\n",
        "from surprise import Reader, Dataset\n",
        "from surprise import SVD, accuracy, SVDpp, SlopeOne, BaselineOnly, CoClustering\n",
        "import datetime\n",
        "import requests, zipfile, io\n",
        "from os import path\n",
        "import pandas as pd\n",
        "import tqdm as tqdm\n",
        "from numpy import *\n",
        "from sklearn.model_selection import train_test_split \n",
        "from collections import defaultdict\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VL1Dk-OfUV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the data \n",
        "rating_data = {}\n",
        "movie_data = {}\n",
        "training_data = []\n",
        "testing_data = []\n",
        "mapping_data = []\n",
        "unique_user_id = []\n",
        "\n",
        "# download http://files.grouplens.org/datasets/movielens/ml-latest-small.zip with 1M records File\n",
        "# all files should be placed inside ml-latest folder\n",
        "if not path.exists('ml-latest-small'):\n",
        "    print(\"Downloading Files for first time use: \")\n",
        "    download_file = requests.get('http://files.grouplens.org/datasets/movielens/ml-latest-small.zip')\n",
        "    zipped_file = zipfile.ZipFile(io.BytesIO(download_file.content)) # having First.csv zipped file.\n",
        "    zipped_file.extractall()\n",
        "\n",
        "\n",
        "# Loading the mapping data which is to map each movie Id\n",
        "# in the ratings with it's title and genre\n",
        "# the resulted data structure is a dictionary where the\n",
        "# movie id is the key, the genre and titles are values\n",
        "def load_mapping_data():\n",
        "    chunk_size = 500000\n",
        "    df_dtype = {\n",
        "        \"movieId\": int,\n",
        "        \"title\": str,\n",
        "        \"genres\": str\n",
        "    }\n",
        "    cols = list(df_dtype.keys())\n",
        "    for df_chunk in tqdm.tqdm(pd.read_csv('ml-latest-small/movies.csv', usecols=cols, dtype=df_dtype, chunksize=chunk_size)):\n",
        "        df_chunk.shape[0]\n",
        "        combine_data = [list(a) for a in\n",
        "                        zip(df_chunk[\"movieId\"].tolist(), df_chunk[\"title\"].tolist(),\n",
        "                            df_chunk[\"genres\"].tolist())]\n",
        "        for a in combine_data:\n",
        "            movie_data[a[0]] = [a[1], a[2]]\n",
        "    del df_chunk\n",
        "\n",
        "# Loading the rating data which is around 27M records it takes around 2 minutes\n",
        "# the resulted data structure us a dictionary where the\n",
        "# user id is the key and all their raings are values for example for user 1 :\n",
        "# 1 = {\n",
        "#     [movieId,rating,timestamp],\n",
        "#     [movieId,rating,timestamp],\n",
        "#     [movieId,rating,timestamp],\n",
        "#   }\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    chunk_size = 50000\n",
        "    df_dtype = {\n",
        "        \"userId\": int,\n",
        "        \"movieId\": int,\n",
        "        \"rating\": float,\n",
        "        \"timestamp\": int,\n",
        "    }\n",
        "    cols = list(df_dtype.keys())\n",
        "    for df_chunk in tqdm.tqdm(pd.read_csv('ml-latest-small/ratings.csv', usecols=cols, dtype=df_dtype, chunksize=chunk_size)):\n",
        "        user_id = df_chunk[\"userId\"].tolist()\n",
        "        unique_user_id.extend(set(user_id))\n",
        "        movie_id = df_chunk[\"movieId\"].tolist()\n",
        "        rating = df_chunk[\"rating\"].tolist()\n",
        "        timestamp = df_chunk[\"timestamp\"].tolist()\n",
        "        combine_data = [list(a) for a in zip(user_id, movie_id, rating, timestamp)]\n",
        "        for a in combine_data:\n",
        "            if a[0] in rating_data.keys():\n",
        "                rating_data[a[0]].extend([[a[0], a[1], a[2], a[3]]])\n",
        "            else:\n",
        "                rating_data[a[0]] = [[a[0], a[1], a[2], a[3]]]\n",
        "    del df_chunk\n",
        "    return(rating_data)\n",
        "\n",
        "# Split the data into training and testing\n",
        "# this processes isn't being done for the whole dataset instead it's being done\n",
        "# for each user id, for each user we split their ratings 80 training and 20 testing\n",
        "# the resulted training and testing datasets are including the whole original dataset\n",
        "\n",
        "def spilt_data():\n",
        "    t0 = time.time()\n",
        "    t1 = time.time()\n",
        "    for u in unique_user_id:\n",
        "        if len(rating_data[u]) == 1:\n",
        "            x_test = rating_data[u]\n",
        "            x_train = rating_data[u]\n",
        "        else:\n",
        "            x_train, x_test = train_test_split(rating_data[u], test_size=0.2)\n",
        "        training_data.extend(x_train)\n",
        "        testing_data.extend(x_test)\n",
        "    total = t1 - t0\n",
        "    print(int(total))\n",
        "\n",
        "def get_movie_title(movie_id):\n",
        "    if movie_id in movie_data.keys():\n",
        "        return movie_data[movie_id][0]\n",
        "\n",
        "def get_movie_genre(movie_id):\n",
        "    if movie_id in movie_data.keys():\n",
        "        return movie_data[movie_id][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTYsjuX7CfrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_traintest_dataframe_forsurprise(training_dataframe, testing_dataframe):\n",
        "    training_dataframe = training_dataframe.iloc[:, :-1]\n",
        "    testing_dataframe = testing_dataframe.iloc[:, :-1]\n",
        "    reader = Reader(rating_scale=(0,5))\n",
        "    trainset = Dataset.load_from_df(training_dataframe[['userId', 'movieId', 'rating']], reader)\n",
        "    testset = Dataset.load_from_df(testing_dataframe[['userId', 'movieId', 'rating']], reader)\n",
        "    trainset = trainset.construct_trainset(trainset.raw_ratings)\n",
        "    testset=testset.construct_testset(testset.raw_ratings)\n",
        "    return([trainset,testset])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EwPzi2CCXgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline(trainset, testset):\n",
        "    algo = BaselineOnly()\n",
        "    algo.fit(trainset)\n",
        "    print(\"Predictions\")\n",
        "    predictions = algo.test(testset)\n",
        "    accuracy.rmse(predictions)\n",
        "    accuracy.mae(predictions)\n",
        "    return(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdq_xqttscnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svdalgorithm(trainset, testset):\n",
        "    algo = SVD()\n",
        "    algo.fit(trainset)\n",
        "    print(\"Predictions\")\n",
        "    predictions = algo.test(testset)\n",
        "    accuracy.rmse(predictions)\n",
        "    accuracy.mae(predictions)\n",
        "    return(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wta1bA9CZ7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def movie_recommendation(predictions, n=10):\n",
        "    # First map the predictions to each user.\n",
        "    algorithmrecommendations_for_each_user = defaultdict(list)\n",
        "    testdaterecommendations_for_each_user = defaultdict(list)\n",
        "    # Creating a dictionary with user_id as the key and the movie_id and the estimated_rating as the value\n",
        "    for user_id, movie_id, true_rating, estimated_rating, _ in predictions:\n",
        "        algorithmrecommendations_for_each_user[user_id].append((movie_id, estimated_rating))\n",
        "        testdaterecommendations_for_each_user[user_id].append((movie_id, true_rating))\n",
        "    # Now we will sort the Estimated_rating of different movies for each user\n",
        "    for user_id, user_ratings in algorithmrecommendations_for_each_user.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        # Filtering the values to top n\n",
        "        algorithmrecommendations_for_each_user[user_id] = user_ratings[:n]\n",
        "    # Now we will sort the True_rating of different movies for each user\n",
        "    for user_id, user_ratings in testdaterecommendations_for_each_user.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        # Filtering the values to top n\n",
        "        testdaterecommendations_for_each_user[user_id] = user_ratings[:n]\n",
        "    return([algorithmrecommendations_for_each_user,testdaterecommendations_for_each_user])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzdGnI0vHgOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precision_recall_calculation(predictions, threshold=3.5):\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_predict_true = defaultdict(list)\n",
        "    for user_id, movie_id, true_rating, predicted_rating, _ in predictions:\n",
        "        user_predict_true[user_id].append((predicted_rating, true_rating))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for user_id, user_ratings in user_predict_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        no_of_relevant_items = sum((true_rating >= threshold) for (predicted_rating, true_rating) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top 10\n",
        "        no_of_recommended_items = sum((predicted_rating >= threshold) for (predicted_rating, true_rating) in user_ratings[:10])\n",
        "\n",
        "        # Number of relevant and recommended items in top 10\n",
        "        no_of_relevant_and_recommended_items = sum(((true_rating >= threshold) and (predicted_rating >= threshold)) for (predicted_rating, true_rating) in user_ratings[:10])\n",
        "\n",
        "        # Precision: Proportion of recommended items that are relevant\n",
        "        precisions[user_id] = no_of_relevant_and_recommended_items / no_of_recommended_items if no_of_recommended_items != 0 else 1\n",
        "\n",
        "        # Recall: Proportion of relevant items that are recommended\n",
        "        recalls[user_id] = no_of_relevant_and_recommended_items / no_of_relevant_items if no_of_relevant_items != 0 else 1\n",
        "\n",
        "    # Averaging the values for all users\n",
        "    average_precision=sum(precision for precision in precisions.values()) / len(precisions)\n",
        "    average_recall=sum(recall for recall in recalls.values()) / len(recalls)\n",
        "    F_score=(2*average_precision*average_recall) / (average_precision + average_recall)\n",
        "    \n",
        "    return [average_precision, average_recall, F_score]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LrEydKJsO9x",
        "colab_type": "code",
        "outputId": "40e0ac47-7e69-4a3d-bef3-84f185a6ca70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Data Loading and Processing, Estimated Time 2 minutes :\")\n",
        "    load_data()\n",
        "    print(\"Training and Testing DataSets Construction, Estimated Time 40 seconds :\")\n",
        "    spilt_data()\n",
        "    print(\"Mapping Data Processing :\")\n",
        "    load_mapping_data()\n",
        "    print(\"Movie name with id = 1 :\")\n",
        "    print(get_movie_title(1))\n",
        "    print(\"Movie genre with id = 1 :\")\n",
        "    print(get_movie_genre(1))\n",
        "    training_dataframe=pd.DataFrame.from_records(training_data)\n",
        "    training_dataframe.columns=[\"userId\",\"movieId\",\"rating\",\"timestamp\"]\n",
        "    testing_dataframe=pd.DataFrame.from_records(testing_data)\n",
        "    testing_dataframe.columns=[\"userId\",\"movieId\",\"rating\",\"timestamp\"]\n",
        "    trainset,testset=convert_traintest_dataframe_forsurprise(training_dataframe,testing_dataframe)\n",
        "    print(\"Baseline algorithm using surprise package\")\n",
        "    baseline(trainset, testset)\n",
        "    print(\"SVD algorithm using surprise package\")\n",
        "    predictions=svdalgorithm(trainset,testset)\n",
        "    algorithm_recommendations,testdata_recommendations = movie_recommendation(predictions, n=10)\n",
        "    # Print the recommended movies for each user\n",
        "    #for user_id, user_ratings in algorithm_recommendations.items():\n",
        "    #  print(user_id, [movie_id for (movie_id, estimated_rating) in user_ratings])\n",
        "    [precision, recall, F_score] = precision_recall_calculation(predictions, threshold=3.5)\n",
        "    print(\"Precision=\", precision)\n",
        "    print(\"Recall=\", recall)\n",
        "    print(\"F-Score=\",F_score)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data Loading and Processing, Estimated Time 2 minutes :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:00,  4.18it/s]\n",
            "1it [00:00, 46.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training and Testing DataSets Construction, Estimated Time 40 seconds :\n",
            "0\n",
            "Mapping Data Processing :\n",
            "Movie name with id = 1 :\n",
            "Toy Story (1995)\n",
            "Movie genre with id = 1 :\n",
            "Adventure|Animation|Children|Comedy|Fantasy\n",
            "Baseline algorithm using surprise package\n",
            "Estimating biases using als...\n",
            "Predictions\n",
            "RMSE: 0.8748\n",
            "MAE:  0.6721\n",
            "SVD algorithm using surprise package\n",
            "Predictions\n",
            "RMSE: 0.8731\n",
            "MAE:  0.6670\n",
            "Precision= 0.7922742648972151\n",
            "Recall= 0.5180123598397629\n",
            "F-Score= 0.6264398244652822\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}